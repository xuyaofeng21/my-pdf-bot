import streamlit as st
import os
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from openai import OpenAI

# --- 1. é¡µé¢åŸºæœ¬è®¾ç½® ---
st.set_page_config(page_title="PDF æ™ºèƒ½é—®ç­”", layout="wide")
st.title("ğŸ“„ PDF æ™ºèƒ½é—®ç­”åŠ©æ‰‹")

# --- 2. ä¾§è¾¹æ ï¼šä¸Šä¼ æ–‡ä»¶ & è®¾ç½® ---

with st.sidebar:
    st.header("1. ä¸Šä¼ æ–‡ä»¶")

    # 1. å°è¯•ä»äº‘ç«¯ Secrets é‡Œæ‹¿ Key
    # æ³¨æ„ï¼šè¿™é‡Œçš„åå­— "DEEPSEEK_API_KEY" å¿…é¡»å’Œä½  Secrets é‡Œå¡«çš„ä¸€æ¨¡ä¸€æ ·
    if "DEEPSEEK_API_KEY" in st.secrets:
        default_key = st.secrets["DEEPSEEK_API_KEY"]
        key_source = "âœ… å·²è‡ªåŠ¨åŠ è½½äº‘ç«¯å¯†é’¥"
    else:
        default_key = ""
        key_source = "âš ï¸ æœªæ£€æµ‹åˆ°äº‘ç«¯å¯†é’¥"

    # 2. æ˜¾ç¤ºçŠ¶æ€æç¤º
    st.caption(key_source)

    # 3. åˆ›å»ºè¾“å…¥æ¡†
    # å¦‚æœæ‰¾åˆ°äº† Secretï¼Œvalue å°±æ˜¯é‚£ä¸ª Keyï¼Œç”¨æˆ·å°±ä¸ç”¨å¡«äº†
    # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œvalue ä¸ºç©ºï¼Œç”¨æˆ·éœ€è¦æ‰‹åŠ¨å¡«
    api_key = st.text_input("DeepSeek API Key", value=default_key, type="password")

    uploaded_file = st.file_uploader("ä¸Šä¼  PDF æ–‡ä»¶", type=["pdf"])

    st.markdown("---")
    st.markdown("### ğŸ› ï¸ å¤„ç†çŠ¶æ€")
    status_text = st.empty()

# --- 3. æ ¸å¿ƒé€»è¾‘ï¼šå¤„ç† PDF (å¦‚æœç”¨æˆ·ä¸Šä¼ äº†æ–°æ–‡ä»¶) ---
# å®šä¹‰ä¸€ä¸ªè·¯å¾„æ¥å­˜æ•°æ®åº“ï¼Œè·Ÿä¹‹å‰çš„åŒºåˆ†å¼€
DB_PATH = "../pdf_chroma_db"


def process_pdf(uploaded_file):
    """è¯»å–PDF -> åˆ‡åˆ† -> å­˜å…¥å‘é‡åº“"""
    # a. å…ˆæŠŠä¸Šä¼ çš„æ–‡ä»¶å­˜æˆä¸´æ—¶æ–‡ä»¶
    temp_file_path = "../temp.pdf"
    with open(temp_file_path, "wb") as f:
        f.write(uploaded_file.getbuffer())

    # b. åŠ è½½ PDF
    loader = PyPDFLoader(temp_file_path)
    docs = loader.load()

    # c. åˆ‡åˆ†æ–‡æ¡£ (Recursive æ˜¯æ›´é«˜çº§çš„åˆ‡åˆ†å™¨ï¼Œä¸ä»…çœ‹å­—æ•°ï¼Œè¿˜çœ‹å¥å·æ®µè½)
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    splits = text_splitter.split_documents(docs)

    # d. å‘é‡åŒ–å¹¶å…¥åº“
    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    # åˆ›å»ºä¸€ä¸ªæ–°çš„æ•°æ®åº“
    vectorstore = Chroma.from_documents(
        documents=splits,
        embedding=embeddings,
        persist_directory=DB_PATH
    )
    return vectorstore


# --- 4. åˆå§‹åŒ– Session State (è®°å¿†) ---
if "messages" not in st.session_state:
    st.session_state.messages = []
if "vector_db" not in st.session_state:
    st.session_state.vector_db = None

# --- 5. åªæœ‰å½“ç”¨æˆ·ç‚¹å‡»ä¸Šä¼ ï¼Œä¸”æ•°æ®åº“æ²¡å‡†å¤‡å¥½æ—¶ï¼Œæ‰å»å¤„ç† ---
if uploaded_file and st.session_state.vector_db is None:
    if not api_key:
        st.error("è¯·å…ˆè¾“å…¥ API Keyï¼")
    else:
        with st.spinner("æ­£åœ¨é˜…è¯» PDFï¼Œè¯·ç¨ç­‰... (ç¬¬ä¸€æ¬¡å¯èƒ½ä¼šä¸‹è½½æ¨¡å‹)"):
            try:
                # è°ƒç”¨ä¸Šé¢çš„å‡½æ•°
                st.session_state.vector_db = process_pdf(uploaded_file)
                st.success("PDF å¤„ç†å®Œæˆï¼ç°åœ¨å¯ä»¥æé—®äº†ã€‚")
            except Exception as e:
                st.error(f"å¤„ç†å¤±è´¥: {e}")

# --- 6. èŠå¤©ç•Œé¢ ---
# æ˜¾ç¤ºå†å²è®°å½•
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.write(msg["content"])

# å¤„ç†ç”¨æˆ·æé—®
user_input = st.chat_input("åœ¨è¿™ä¸ª PDF é‡Œæ‰¾ä»€ä¹ˆï¼Ÿ")

if user_input:
    # æ£€æŸ¥æœ‰æ²¡æœ‰ Key å’Œ æ•°æ®åº“
    if not api_key:
        st.warning("è¯·å…ˆè®¾ç½® API Key")
        st.stop()
    if st.session_state.vector_db is None:
        st.warning("è¯·å…ˆä¸Šä¼  PDF æ–‡ä»¶")
        st.stop()

    # A. æ˜¾ç¤ºç”¨æˆ·é—®é¢˜
    with st.chat_message("user"):
        st.write(user_input)
    st.session_state.messages.append({"role": "user", "content": user_input})

    # B. æ ¸å¿ƒ RAG æ£€ç´¢æµç¨‹
    with st.chat_message("assistant"):
        with st.spinner("AI æ­£åœ¨ç¿»ä¹¦æŸ¥æ‰¾..."):
            # 1. åœ¨æ•°æ®åº“é‡Œæœ
            db = st.session_state.vector_db
            docs = db.similarity_search(user_input, k=2)  # æ‰¾æœ€ç›¸ä¼¼çš„2ä¸ªç‰‡æ®µ

            if not docs:
                context = "æ²¡æœ‰åœ¨æ–‡æ¡£ä¸­æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚"
            else:
                # æŠŠæ‰¾åˆ°çš„æ–‡å­—æ‹¼èµ·æ¥
                context = "\n\n".join([d.page_content for d in docs])

            # 2. ç»„è£… Prompt
            prompt = f"""
            ä½ æ˜¯ä¸€ä¸ªæ–‡æ¡£åŠ©æ‰‹ã€‚åŸºäºä»¥ä¸‹ã€å‚è€ƒèµ„æ–™ã€‘å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

            ã€å‚è€ƒèµ„æ–™ã€‘ï¼š
            {context}

            ã€ç”¨æˆ·é—®é¢˜ã€‘ï¼š
            {user_input}
            """

            # 3. è°ƒç”¨ DeepSeek
            client = OpenAI(api_key=api_key, base_url="https://api.deepseek.com")
            response = client.chat.completions.create(
                model="deepseek-chat",
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¹äºåŠ©äººçš„åŠ©æ‰‹ã€‚"},
                    {"role": "user", "content": prompt}
                ]
            )

            # 4. æ˜¾ç¤ºç­”æ¡ˆ
            answer = response.choices[0].message.content
            st.write(answer)

            # 5. æ—¢ç„¶æ˜¯ RAGï¼Œæœ€å¥½å±•ç¤ºä¸€ä¸‹å‚è€ƒäº†å“ªä¸€æ®µï¼ˆæ˜¾å¾—ä¸“ä¸šï¼‰
            with st.expander("æŸ¥çœ‹ AI å‚è€ƒçš„åŸæ–‡ç‰‡æ®µ"):
                st.write(context)

            st.session_state.messages.append({"role": "assistant", "content": answer})